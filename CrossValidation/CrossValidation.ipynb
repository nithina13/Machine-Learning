{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crossvalidation and hyperparameter selection  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries! If you want to add things here for visualization, please do, \n",
    "# but only use sklearn when prompted\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree \n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import load_diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Calculate Generalized Error on Linear Regression with k-fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.1 Load in the diabetes data set as a pandas dataframe and series.  \n",
    "Documentation is [here](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html).  Name your features dataframe (the independent variables) `df_X` and your target (the dependent variable) series `s_y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      151.0\n",
       "1       75.0\n",
       "2      141.0\n",
       "3      206.0\n",
       "4      135.0\n",
       "       ...  \n",
       "437    178.0\n",
       "438    104.0\n",
       "439    132.0\n",
       "440    220.0\n",
       "441     57.0\n",
       "Name: target, Length: 442, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X, temp = load_diabetes(return_X_y=True, as_frame=True)\n",
    "s_y = temp.squeeze()\n",
    "s_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.2 Define a function that creates a linear least squares regression model \n",
    "This function should take in two parameters, `df_X`, and `s_y` and return the least squares regression model, $\\hat{beta}$ (using the notation from the ESLII book chapter 3 and HW2).  You can not use any libraries outside of pandas and numpy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     152.133484\n",
       "1     -10.012198\n",
       "2    -239.819089\n",
       "3     519.839787\n",
       "4     324.390428\n",
       "5    -792.184162\n",
       "6     476.745838\n",
       "7     101.044570\n",
       "8     177.064176\n",
       "9     751.279321\n",
       "10     67.625386\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def leastSquares(df_X, s_y):\n",
    "    \n",
    "    # Insert the column of ones\n",
    "    df_X_1 = df_X.copy()\n",
    "    df_X_1.insert(0, '', 1)\n",
    "\n",
    "    # Get the y vector\n",
    "    y_data = s_y.to_numpy()\n",
    "\n",
    "    # One-liner to calculate beta_hat\n",
    "    beta_hat = np.linalg.inv(df_X_1.transpose() @ df_X_1) @ df_X_1.transpose() @ y_data\n",
    "    \n",
    "    return beta_hat\n",
    "leastSquares(df_X, s_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.3 Define a function that partitions the dataframe and series data into dictionaries\n",
    "This function should take in three parameters, `df_X`, `s_y`, and `k`, and returns a tuple of two dictionaries.\n",
    "In both dictionaries the key is the `k` value (an integer from one to `k` inclusive).\n",
    "The first dictionary will have the dataframe of the training data that contains approximately $\\frac{1}{k}$ of the data (variation due to randomness is acceptable).\n",
    "The second dictionary will have the series of the target data that contains approximately $\\frac{1}{k}$ of the data (variation due to randomness is acceptable). Please note the targets _must match_ the same rows as the dataframe at this index, e.g, the length of the kth partition is the same for the dataframe and series.\n",
    "\n",
    "Call that function with $k=5$ and create the dictionaries `dict_k_df_X` and `dict_k_s_y`. Print out the number of rows in each fold.  Check that the number of data points in each partition totals the number of data points in the entire dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_k_df_X | fold number 1 | num rows: 102\n",
      "dict_k_df_X | fold number 2 | num rows: 83\n",
      "dict_k_df_X | fold number 3 | num rows: 78\n",
      "dict_k_df_X | fold number 4 | num rows: 95\n",
      "dict_k_df_X | fold number 5 | num rows: 84\n",
      "sum dict_k_df_X = 442\n",
      "\n",
      "-------------------------\n",
      "\n",
      "dict_k_s_y | fold number 1 | num rows: 102\n",
      "dict_k_s_y | fold number 2 | num rows: 83\n",
      "dict_k_s_y | fold number 3 | num rows: 78\n",
      "dict_k_s_y | fold number 4 | num rows: 95\n",
      "dict_k_s_y | fold number 5 | num rows: 84\n",
      "sum dict_k_s_y = 442\n"
     ]
    }
   ],
   "source": [
    "def getDataFolds(df_X, s_y, k):\n",
    "    \n",
    "    # make list size lenght(df_X) with random numbers between 1 and k\n",
    "    random_assignments = np.random.randint(1, k + 1, size=df_X.shape[0])\n",
    "    \n",
    "    # add this list as a column to both dfs\n",
    "    df_X = df_X.join(pd.DataFrame(random_assignments, columns=['fold_n']))\n",
    "    df_sy = s_y.to_frame()\n",
    "    df_sy = df_sy.join(pd.DataFrame(random_assignments, columns=['fold_n']))\n",
    "    \n",
    "    # create a dictionary with keys from the fold_n columns, but drop fold_n\n",
    "    keys = [i for i in range(1, k+1)]\n",
    "    dict_k_df_X = dict(tuple(df_X.groupby('fold_n')))\n",
    "    dict_k_s_y = dict(tuple(df_sy.groupby('fold_n')))\n",
    "    \n",
    "    # drop the fold number from each dataframe\n",
    "    dict_k_df_X = dict(zip(keys,[x.drop(columns=['fold_n']) for x in dict_k_df_X.values()])) \n",
    "    dict_k_s_y = dict(zip(keys,[x.drop(columns=['fold_n']) for x in dict_k_s_y.values()]))\n",
    "    \n",
    "    return dict_k_df_X, dict_k_s_y\n",
    "        \n",
    "# sniff\n",
    "x_dict,y_dict = getDataFolds(df_X, s_y, 5)\n",
    "\n",
    "s = 0\n",
    "for i in range(1, 6):\n",
    "    s += x_dict[i].shape[0]\n",
    "    print(\"dict_k_df_X | fold number {} | num rows: {}\".format(i, x_dict[i].shape[0]))\n",
    "print(\"sum dict_k_df_X = {}\".format(s))\n",
    "\n",
    "print(\"\\n-------------------------\\n\")\n",
    "\n",
    "s = 0\n",
    "for i in range(1, 6):\n",
    "    s += y_dict[i].shape[0]\n",
    "    print(\"dict_k_s_y | fold number {} | num rows: {}\".format(i, y_dict[i].shape[0]))\n",
    "print(\"sum dict_k_s_y = {}\".format(s))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.4 Define a function that calculates a regression metric\n",
    "This function should accept two series of equal length $n$ numpy arrays, `s_y`, and `s_y_hat`. The metric it should calculate is the mean absolute error, $MAE = \\sum\\limits_{i=1}^n\\frac{|{s\\_y_i - {s\\_y\\_hat}_i}|}{n}$ \n",
    "\n",
    "Test your function by using the vectors:\n",
    "```\n",
    "x = np.array([1,2,3])\n",
    "y = np.array([2,2,3])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def MAE(s_y, s_y_hat):\n",
    "    return np.sum(np.abs(s_y - s_y_hat) / s_y.shape[0])\n",
    "\n",
    "\n",
    "x = np.array([1,2,3])\n",
    "y = np.array([2,2,3])\n",
    "MAE(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.5 Calculate the $MAE$ for each fold\n",
    "For each fold in your dictionaries, calculate the $MAE$.  Use the partition number key as the test set, and all other partitions as the train set. \n",
    "\n",
    "Print the min, max, and mean $MAE$ of your 5 folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          age       sex       bmi        bp        s1        s2        s3  \\\n",
      "12   0.016281 -0.044642 -0.028840 -0.009113 -0.004321 -0.009769  0.044958   \n",
      "13   0.005383  0.050680 -0.001895  0.008101 -0.004321 -0.015719 -0.002903   \n",
      "17   0.070769  0.050680  0.012117  0.056301  0.034206  0.049416 -0.039719   \n",
      "29   0.067136  0.050680 -0.006206  0.063187 -0.042848 -0.095885  0.052322   \n",
      "34   0.016281 -0.044642 -0.063330 -0.057314 -0.057983 -0.048912  0.008142   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "417 -0.052738 -0.044642  0.071397 -0.074528 -0.015328 -0.001314  0.004460   \n",
      "419 -0.020045 -0.044642 -0.054707 -0.053871 -0.066239 -0.057367  0.011824   \n",
      "423  0.009016  0.050680 -0.039618  0.028758  0.038334  0.073529 -0.072854   \n",
      "426  0.030811  0.050680 -0.034229  0.043677  0.057597  0.068831 -0.032356   \n",
      "429 -0.041840 -0.044642 -0.033151 -0.022885  0.046589  0.041587  0.056003   \n",
      "\n",
      "           s4        s5        s6  \n",
      "12  -0.039493 -0.030751 -0.042499  \n",
      "13  -0.002592  0.038393 -0.013504  \n",
      "17   0.034309  0.027368 -0.001078  \n",
      "29  -0.076395  0.059424  0.052770  \n",
      "34  -0.039493 -0.059473 -0.067351  \n",
      "..        ...       ...       ...  \n",
      "417 -0.021412 -0.046879  0.003064  \n",
      "419 -0.039493 -0.074089 -0.005220  \n",
      "423  0.108111  0.015567 -0.046641  \n",
      "426  0.057557  0.035462  0.085907  \n",
      "429 -0.024733 -0.025952 -0.038357  \n",
      "\n",
      "[104 rows x 10 columns]\n",
      "Min MAE: 36.995165269382184\n",
      "MaX MAE: 52.09234129740995 \n",
      "Average MAE: 44.078491095405745\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def crossValidate(df_x, s_y, k):\n",
    "    \n",
    "    # get the folded data\n",
    "    dict_k_df_X, dict_k_df_y = getDataFolds(df_x, s_y, k)\n",
    "    \n",
    "    # store MAEs\n",
    "    MAEs = []\n",
    "    \n",
    "    # for every fold (k)\n",
    "    for i in range(1, k+1):\n",
    "        \n",
    "        # get the test set\n",
    "        test_X_df = dict_k_df_X[i]\n",
    "        test_y_df = dict_k_df_y[i]\n",
    "        \n",
    "        # create the train set\n",
    "        train_X_df = pd.DataFrame()\n",
    "        train_y_df = pd.DataFrame()\n",
    "        for j in range(1, k+1):\n",
    "            \n",
    "            # skip the test set\n",
    "            if (j != i):\n",
    "                train_X_df = train_X_df.append(dict_k_df_X[j])\n",
    "                train_y_df = train_y_df.append(dict_k_df_y[j])\n",
    "\n",
    "        # calculate beta-hat\n",
    "        beta_hat = leastSquares(train_X_df, train_y_df.squeeze())\n",
    "        \n",
    "        # calculate s_y_hat predictions\n",
    "        s_y_hat = beta_hat[0] + test_X_df.to_numpy().dot(beta_hat[1:])\n",
    "\n",
    "        # calculate MAE\n",
    "        MAEs.append(MAE(test_y_df.squeeze().to_numpy(), s_y_hat))\n",
    "    \n",
    "    print(test_X_df)\n",
    "    return min(MAEs), max(MAEs), np.mean(MAEs)\n",
    "        \n",
    "minMAE, maxMAE, avgMAE = crossValidate(df_X, s_y, 5)\n",
    "\n",
    "print(\"Min MAE: {}\\nMaX MAE: {} \\nAverage MAE: {}\\n\".format(minMAE, maxMAE, avgMAE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Find the best hyperparameter to use in a Decision Tree "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2.1 Load the iris data in as a pandas dataframe and a series\n",
    "Documentation is [here](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html). Name your features dataframe (the independent variables) `df_X` and your classification target (the dependent variable) series `s_y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X, temp = load_iris(return_X_y=True, as_frame=True)\n",
    "s_y = temp.squeeze()\n",
    "s_y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2.2 Partition `df_X` and `s_y` into $5$ partitions of roughly equal size\n",
    "Make 2 dictionaries, with the key of each dictionary the fold number.  The value of the dictionary `dict_df_X` is the $k^{th}$ partition of the data, and the value of the dictionary `dict_s_y` is the corresponding $k^{th}$ target classification.  Print out the number of rows in each fold.  Check that the number of data points in each partition totals the number of data points in the entire dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df_X, dict_s_y = getDataFolds(df_X, s_y, 5)\n",
    "s = 0\n",
    "for i in range(1, 6):\n",
    "    s += dict_df_X[i].shape[0]\n",
    "    print(\"dict_df_X | fold number {} | num rows: {}\".format(i, dict_df_X[i].shape[0]))\n",
    "print(\"sum dict_df_X = {}\".format(s))\n",
    "\n",
    "print(\"\\n-------------------------\\n\")\n",
    "\n",
    "s = 0\n",
    "for i in range(1, 6):\n",
    "    s += dict_s_y[i].shape[0]\n",
    "    print(\"dict_s_y | fold number {} | num rows: {}\".format(i, dict_s_y[i].shape[0]))\n",
    "print(\"sum dict_s_y = {}\".format(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2.3 Define a function that calculates accuracy\n",
    "The function should accept two series and compare each element for equality.  The accuracy is the number of equal elements divided by the total number of elements.\n",
    "\n",
    "Test your accuracy function by calling it with the `s_y` loaded from the iris data set and an array of the same length containing all $1$ values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAccuracy(s_y_hat, s_y_truth):\n",
    "    return np.count_nonzero(s_y_truth.to_numpy()==s_y_hat.to_numpy()) / np.size(s_y_hat.to_numpy())\n",
    "\n",
    "\n",
    "ones = pd.Series(np.ones(s_y.shape[0]))\n",
    "getAccuracy(ones, s_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2.4 Using Nested Cross validation, find the best hyperparameter\n",
    "Use the [Decision Tree Classifier](https://scikit-learn.org/stable/modules/tree.html#classification) class to build a decision tree inside of a 5-fold cross validation loop.  The partitions you already created in 2.2 will be the outer loop.  In the inside loop you should use 4-fold cross validation (so you don't have to partition _again_) to find the best value for `min_impurity_decrease`.  Use the Gini Index as your impurity measure. \n",
    "    Calculate the mean accuracy across the 4 folds of your inner loop for all the candidate `min_impurity_decrease` values, and print the value.  Use the array `np.array([0.1,0.25,0.3,0.4])` to search for the best hyperparameter. \n",
    "\n",
    "For each inner loop, select the best `min_impurity_decrease` and train the outer fold training data on using that value. \n",
    "\n",
    "For each inner loop, your output should look something like this:\n",
    "```\n",
    "Testing 0.10 min impurity decrease\n",
    "\tAverage accuracy over 4 folds is 0.95\n",
    "Testing 0.25 min impurity decrease\n",
    "\tAverage accuracy over 4 folds is 0.86\n",
    "Testing 0.30 min impurity decrease\n",
    "\tAverage accuracy over 4 folds is 0.63\n",
    "Testing 0.40 min impurity decrease\n",
    "\tAverage accuracy over 4 folds is 0.27\n",
    "\n",
    "Best min impurity decrease is 0.1\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 outer folds\n",
    "k = 5\n",
    "\n",
    "# get the folded data\n",
    "dict_df_X, dict_s_y = getDataFolds(df_X, s_y, k)\n",
    "\n",
    "# store strings to print\n",
    "crnt_imp_str = \"Testing {} min impurity decrease\"\n",
    "avg_acc_str = \"\\tAverage accuracy over 4 folds is {}\"\n",
    "best_str = \"\\nBest min impurity decrease is {}\\n\"\n",
    "    \n",
    "    \n",
    "# Q2.5, outer-fold accuracies for test and train sets.\n",
    "test_acc = []\n",
    "train_acc = []\n",
    "\n",
    "# for every fold (k)\n",
    "for i in range(1, k+1):\n",
    "\n",
    "    # create 4 trees\n",
    "    trees = []\n",
    "    hp_MID = np.array([0.1,0.25,0.3,0.4])\n",
    "    for hyper_param in hp_MID:\n",
    "        trees.append(tree.DecisionTreeClassifier(min_impurity_decrease=hyper_param, criterion = \"gini\"))\n",
    "    \n",
    "    # get the test set\n",
    "    test_X_df = dict_df_X[i]\n",
    "    test_y_df = dict_s_y[i]\n",
    "\n",
    "    # create the train set (4 folds)\n",
    "    train_X_df = pd.DataFrame()\n",
    "    train_y_df = pd.DataFrame()\n",
    "    for j in range(1, k+1):\n",
    "        # skip the test set\n",
    "        if (j != i):\n",
    "            train_X_df = train_X_df.append(dict_df_X[j])\n",
    "            train_y_df = train_y_df.append(dict_s_y[j])\n",
    "\n",
    "    \n",
    "    accuracies = []\n",
    "    # train each tree\n",
    "    for t in trees:\n",
    "        \n",
    "        print(crnt_imp_str.format(t.min_impurity_decrease))\n",
    "        \n",
    "        # train tree\n",
    "        t.fit(train_X_df, train_y_df)\n",
    "        \n",
    "        # predict each tree\n",
    "        t_yhat = t.predict(test_X_df.to_numpy())\n",
    "    \n",
    "        # calculate accuracy\n",
    "        acc = getAccuracy(pd.Series(t_yhat), test_y_df.squeeze())\n",
    "        accuracies.append(acc)\n",
    "        \n",
    "        print(avg_acc_str.format(round(acc, 2)))\n",
    "    \n",
    "    # get highest accuracy tree\n",
    "    h_idx = accuracies.index(max(accuracies))\n",
    "    print(best_str.format(hp_MID[h_idx]))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2.5 Show the generalized performance of the classifier \n",
    "Show the generalized performance of the classifier by printing the min, max, and mean accuracy of the outer fold test and train sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on the previous experiment, a min_impurity_decrease of 0.1 was \n",
    "# determined to be the best hyperparamter. \n",
    "# The min, max, mean accuracies accross 5-folds is shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 outer folds\n",
    "k = 5\n",
    "\n",
    "# get the folded data\n",
    "dict_df_X, dict_s_y = getDataFolds(df_X, s_y, k)\n",
    "\n",
    "# Create tree using best hyper parameter\n",
    "t = tree.DecisionTreeClassifier(min_impurity_decrease=0.1, criterion = \"gini\")\n",
    "    \n",
    "\n",
    "train_accuracies = []\n",
    "# for every fold (k)\n",
    "for i in range(1, k+1):\n",
    "    \n",
    "    # get the test set\n",
    "    test_X_df = dict_df_X[i]\n",
    "    test_y_df = dict_s_y[i]\n",
    "\n",
    "    # create the train set (4 folds)\n",
    "    train_X_df = pd.DataFrame()\n",
    "    train_y_df = pd.DataFrame()\n",
    "    for j in range(1, k+1):\n",
    "        # skip the test set\n",
    "        if (j != i):\n",
    "            train_X_df = train_X_df.append(dict_df_X[j])\n",
    "            train_y_df = train_y_df.append(dict_s_y[j])\n",
    "\n",
    "\n",
    "    # train tree\n",
    "    t.fit(train_X_df, train_y_df)\n",
    "\n",
    "    # predict the tree\n",
    "    t_yhat = t.predict(test_X_df.to_numpy())\n",
    "\n",
    "    # calculate accuracy\n",
    "    acc = getAccuracy(pd.Series(t_yhat), test_y_df.squeeze())\n",
    "    train_accuracies.append(acc)\n",
    "\n",
    "print(\"--Generalized Performance--\")\n",
    "print(\"Min : {}\\nMax : {}\\nMean: {}\".format(min(train_accuracies), max(train_accuracies), sum(train_accuracies)/len(train_accuracies)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
